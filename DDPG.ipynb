{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control in a continuous action space with DDPG\n",
    "_Authors:_ Aristotelis Dimitriou, Konstantinos Spinakis\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this reinforcement learning project, we implement the Deep Deterministic Policy Gradient (DDPG) algorithm to handle continuous action spaces while maintaining the benefits of Deep Q-learning (DQN). The objective is to stabilize an inverted pendulum in the Pendulum-v1 environment from OpenAI Gym.\n",
    "\n",
    "DDPG is an actor-critic algorithm that utilizes one neural network (critic) to estimate the Q function and another (actor) to select the action. It is based on the deterministic policy gradient theorem, allowing both the actor and critic to be trained off-policy from a replay buffer. The policy network outputs a specific action instead of a probability distribution, enabling a flexible exploration strategy.\n",
    "\n",
    "* The `Pendulum-v1` environment provides a three-dimensional observation vector $(\\cos(\\alpha), \\sin(\\alpha), \\dot{\\alpha})$ where $\\alpha$ represents the angle between the pendulum and the vertical line. \n",
    "\n",
    "* The action is a scalar value between -2 and 2, representing the torque applied to the pendulum's unique joint. \n",
    "\n",
    "* The control policy must learn to swing the pendulum to gain momentum before stabilizing it in a vertical position with minimal torque. \n",
    "\n",
    "* The reward function is defined as $-(\\alpha^2 + 0.1\\cdot\\dot{\\alpha}^2 + 0.001\\cdot\\tau^2)$, with the maximum reward of 0 achieved when the pendulum is vertically positioned, motionless, and with no torque applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Heuristic Policy\n",
    "\n",
    "In this section, we will familiarize ourselves with the `Pendulum-v1` environment by implementing a simple heuristic policy to attempt stabilizing the pendulum. We will compare the heuristic policy with a random policy to verify the increase in average reward.\n",
    "\n",
    "_**Tasks:**_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Create an instance of the `Pendulum-v1` environment and wrap it in a `NormalizedEnv` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NormalizedEnv(gym.make('Pendulum-v1'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement a functions that simulates an interaction between the environment and the agent. Returning the average cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent, env, episodes=10, verbose=False):\n",
    "    rewards = []\n",
    "    for i in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.compute_action(state)\n",
    "            next_state, reward, _, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if verbose:\n",
    "                print(f'Episode {i+1}/{episodes}')\n",
    "                print(f'State: {state}')\n",
    "                print(f'Action: {action}')\n",
    "                print(f'Reward: {reward}')\n",
    "                print(f'Done: {done}')\n",
    "                print('------------------')\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement a heuristic policy for the pendulum (`HeuristicPendulumAgent`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeuristicPendulumAgent:\n",
    "    def __init__(self, env, fixed_torque=0.5, verbose=False):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.fixed_torque = fixed_torque\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def compute_action(self, state):\n",
    "        if self.verbose:\n",
    "            print(f'State: {state}')\n",
    "        x, y, angular_velocity = state\n",
    "        if y < 0:   # Lower half of the domain\n",
    "            action = np.sign(angular_velocity) * self.fixed_torque\n",
    "        else:       # Upper half of the domain\n",
    "            action = -np.sign(angular_velocity) * self.fixed_torque\n",
    "        return np.array([action])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compare the average cumulative reward obtained by the heuristic policy and compare it with the reward of the random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random agent average reward: -1024.38\n",
      "Heuristic agent average reward: -1173.38\n"
     ]
    }
   ],
   "source": [
    "heuristic_agent = HeuristicPendulumAgent(env, verbose=False)\n",
    "random_agent = RandomAgent(env)\n",
    "\n",
    "heuristic_agent_avg_reward = run_agent(heuristic_agent, env, verbose=False)\n",
    "random_agent_avg_reward = run_agent(random_agent, env, verbose=False)\n",
    "print(f'Random agent average reward: {random_agent_avg_reward:.2f}')\n",
    "print(f'Heuristic agent average reward: {heuristic_agent_avg_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\" A buffer for storing transitions sampled from the environment. \"\"\"\n",
    "    def __init__(self, max_size, verbose=False):\n",
    "        self.max_size = max_size\n",
    "        self.transitions = []\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def store(self, transition):\n",
    "        \"\"\" Store a transition. \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Storing transition {transition}')\n",
    "            \n",
    "        if len(self.transitions) < self.max_size:\n",
    "            self.transitions.append(transition)\n",
    "        else:\n",
    "            self.transitions.pop(0)\n",
    "            self.transitions.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Sample a batch of transitions. \"\"\"\n",
    "        batch = []\n",
    "        for _ in range(batch_size):\n",
    "            idx = np.random.randint(0, len(self.transitions))\n",
    "            batch.append(self.transitions[idx])\n",
    "            if self.verbose:\n",
    "                print(f'Sampling transition {self.transitions[idx]}')\n",
    "        return batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(4, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of the network. \"\"\"\n",
    "        x = x.view(-1, 4)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_network(q_network, transitions, optimizer, gamma):\n",
    "    \"\"\"\n",
    "    Train Q-Network using 1-step TD-learning rule.\n",
    "\n",
    "    Parameters:\n",
    "    q_network (QNetwork): The Q-Network instance to be trained.\n",
    "    transitions (list): A list of tuples containing the transitions used for training.\n",
    "                        Each tuple should have the format (state, action, reward, next_state, trunc).\n",
    "    optimizer (torch.optim.Optimizer): The optimizer used for updating the Q-Network's weights.\n",
    "    gamma (float): The discount factor for future rewards (0 <= gamma <= 1).\n",
    "\n",
    "    Returns:\n",
    "    loss (float): The loss value after training the Q-Network with the given batch of transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack transitions\n",
    "    states, actions, rewards, next_states, trunc = zip(*transitions)\n",
    "    print(f'States: {states}')\n",
    "    print(f'Actions: {actions}')\n",
    "    print(f'Rewards: {rewards}')\n",
    "    print(f'Next states: {next_states}')\n",
    "    print(f'Trunc: {trunc}')\n",
    "    \n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    trunc = torch.tensor(trunc, dtype=torch.float32)\n",
    "\n",
    "    # Conacatenate states and actions to form input to Q-network\n",
    "    state_action_pair = torch.cat((states, actions), dim=-1)\n",
    "    print(f'State-action pair: {state_action_pair}')\n",
    "    \n",
    "\n",
    "    # Computes Q-values for the given state-action pairs\n",
    "    #   Use `QNetwork` class\n",
    "    peos = q_network(state_action_pair)\n",
    "    print(f'Predicted Q-values: {peos}')\n",
    "\n",
    "    # Compute target Q-values\n",
    "    #   Use torck.no_grad() or something like that\n",
    "\n",
    "    # Compute loss\n",
    "    #   Use MSE\n",
    "    # torch.nn.functional.mse_loss\n",
    "\n",
    "    # Perform backpropagation\n",
    "    #   zero_grad() -> backward() -> step()\n",
    "\n",
    "    # return loss.item()\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: (array([-0.03370433, -0.99943185,  0.3547029 ], dtype=float32), array([-0.88589674, -0.46388254, -0.75104225], dtype=float32), array([ 0.7519081 ,  0.6592679 , -0.03331237], dtype=float32))\n",
      "Actions: (array([0.24059471]), array([0.11533584]), array([0.08065662]))\n",
      "Rewards: (-2.5872557846275797, -7.127909880672323, -0.5183133532392202)\n",
      "Next states: (array([-0.0498247 , -0.99875796, -0.32269257], dtype=float32), array([-0.9093176, -0.4161027, -1.0643535], dtype=float32), array([0.73569   , 0.6773184 , 0.48533553], dtype=float32))\n",
      "Trunc: (False, False, False)\n",
      "State-action pair: tensor([[-0.0337, -0.9994,  0.3547,  0.2406],\n",
      "        [-0.8859, -0.4639, -0.7510,  0.1153],\n",
      "        [ 0.7519,  0.6593, -0.0333,  0.0807]])\n",
      "Predicted Q-values: tensor([[0.0725],\n",
      "        [0.1138],\n",
      "        [0.1549]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Make variables to be given to the train_q_network function\n",
    "q_network = QNetwork()\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=0.001)\n",
    "gamma = 0.99\n",
    "\n",
    "# Make a replay buffer\n",
    "replay_buffer = ReplayBuffer(max_size=1000, verbose=False)\n",
    "\n",
    "# Make a random agent\n",
    "random_agent = RandomAgent(env)\n",
    "\n",
    "# Collect some transitions\n",
    "transitions = []\n",
    "for _ in range(3):\n",
    "    state, _,  = env.reset()\n",
    "    action = random_agent.compute_action(state)\n",
    "    next_state, reward, _, done, _ = env.step(action)\n",
    "    transition = (state, action, reward, next_state, done)\n",
    "    transitions.append(transition)\n",
    "    replay_buffer.store(transition)\n",
    "\n",
    "\n",
    "train_q_network(q_network, transitions, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(q_network, action_range, velocity_range, n_points):\n",
    "    \"\"\"Create heatmap of Q-values for the pendulum environment.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
